{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.data import load_synth\n",
    "\n",
    "import random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activations:\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(gamma):\n",
    "        if gamma < 0.0:\n",
    "            return 1.0 - 1.0 / (1.0 + float(math.exp(gamma)))\n",
    "\n",
    "        return 1.0 / (1.0 + float(math.exp(-gamma)))\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(current, values):\n",
    "        denominator = 0.0\n",
    "\n",
    "        for i in range(len(values)):\n",
    "            denominator += float(math.exp(values[i]))\n",
    "\n",
    "        return float(math.exp(current)) / denominator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Losses:\n",
    "\n",
    "    @staticmethod\n",
    "    def cross_entropy(predicted, targets):\n",
    "        loss = 0.0\n",
    "\n",
    "        for i in range(0, len(predicted)):\n",
    "            loss += float(targets[i]) * float(math.log(predicted[i]))\n",
    "        \n",
    "        return  -1.0 * loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Img] (\"files/neural_net.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "        self.T = None\n",
    "        self.initialize()\n",
    "    \n",
    "    def predict(self, X: list) -> [ float, float ]:\n",
    "        self.X = X\n",
    "        return self.forwards()\n",
    "\n",
    "    def set_input(self, input: list) -> None:\n",
    "        self.X = input\n",
    "\n",
    "    def set_target(self, target: list) -> None:\n",
    "        self.T = target\n",
    "   \n",
    "    def set_learning_rate(self, learning_rate: int) -> None:\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def initialize_weight(self) -> float:\n",
    "        return random.gauss(0, 1)\n",
    "    \n",
    "    def initialize(self) -> None:\n",
    "        # First set of weights mapping X to K\n",
    "        self.W = [\n",
    "          [ self.initialize_weight(), self.initialize_weight(), self.initialize_weight() ],\n",
    "          [ self.initialize_weight(), self.initialize_weight(), self.initialize_weight() ]\n",
    "        ]\n",
    "\n",
    "        # Bias for the first layer\n",
    "        self.B = [ 0.0, 0.0, 0.0 ]\n",
    "\n",
    "        # Linear outputs for the first hidden layer\n",
    "        self.K = [ 0.0, 0.0, 0.0 ]\n",
    "\n",
    "        # Output of the sigmoid function applied to K, H = sigmoid(K)\n",
    "        self.H = [ 0.0, 0.0, 0.0 ]\n",
    "\n",
    "        # Second set of weights mapping H to the linear output being fed to the Softmax function\n",
    "        self.V = [\n",
    "            [ self.initialize_weight(), self.initialize_weight() ], \n",
    "            [ self.initialize_weight(), self.initialize_weight() ], \n",
    "            [ self.initialize_weight(), self.initialize_weight() ]\n",
    "        ]\n",
    "\n",
    "        # Second set of biases\n",
    "        self.C = [ 0.0, 0.0 ]\n",
    "\n",
    "        # Linear output fed to the Softmax function\n",
    "        self.O = [ 0.0, 0.0 ]\n",
    "\n",
    "        # One hot encoded output of the softmax function\n",
    "        self.Y = [ 0.0, 0.0 ]\n",
    "\n",
    "    def forwards(self) -> None:\n",
    "        # Calculate K = X * W + B\n",
    "        for i in range(len(self.X)):\n",
    "            for j in range(len(self.W[0])):\n",
    "                self.K[j] += self.X[i] * self.W[i][j]\n",
    "\n",
    "        for i in range(len(self.B)):\n",
    "            self.K[i] += self.B[i]\n",
    "\n",
    "        # Calculate H = sigmoid(K)\n",
    "        for i in range(len(self.K)):\n",
    "            self.H[i] = Activations.sigmoid(self.K[i])\n",
    "\n",
    "        # Calculate O = H * V + C\n",
    "        for i in range(len(self.H)):\n",
    "            for j in range(len(self.V[0])):\n",
    "                self.O[j] += self.H[i] * self.V[i][j]\n",
    "\n",
    "        for i in range(len(self.C)):\n",
    "            self.O[i] += self.C[i]\n",
    "\n",
    "        # Calculate Y = softmax(O)\n",
    "        self.Y = [ Activations.softmax(self.O[0], self.O), Activations.softmax(self.O[1], self.O) ]\n",
    "\n",
    "        # return the predicted value\n",
    "        return self.Y\n",
    "\n",
    "    def calculate_gradients(self) -> None:\n",
    "        try:\n",
    "#             @todo it doesn't seemt like this value actually ever gets used \n",
    "#             self.dLdY = [0.0, 0.0]\n",
    "\n",
    "#             # Calculate the derivative of the loss (L) wrt to Y\n",
    "#             for i in range(len(self.Y)):\n",
    "#                 self.dLdY[i] = -1.0 * float(self.T[i]) / float(self.Y[i])\n",
    "\n",
    "            # Calculate the gradient of the loss (L) wrt to the linear outputs\n",
    "            self.dLdO = [0.0, 0.0]\n",
    "            for i in range(len(self.O)):\n",
    "                self.dLdO[i] = self.Y[i] - self.T[i]\n",
    "\n",
    "            # Calculate the gradient of the loss (L) wrt to the bias C\n",
    "            self.dLdC = self.dLdO\n",
    "\n",
    "            # Calculate the gradient of the loss (L) wrt to the hidden layer H\n",
    "            self.dLdH = [0.0, 0.0, 0.0]\n",
    "\n",
    "            for i in range(len(self.V)):\n",
    "                for j in range(len(self.O)):\n",
    "                    self.dLdH[i] += self.dLdO[j] * self.V[i][j]\n",
    "\n",
    "            # Calculate the gradient of the loss (L) wrt to weights V of the hidden layer H\n",
    "            self.dLdV = [\n",
    "              [0.0, 0.0],\n",
    "              [0.0, 0.0],\n",
    "              [0.0, 0.0]\n",
    "            ]\n",
    "\n",
    "            for i in range(len(self.H)):\n",
    "                for j in range(len(self.O)):\n",
    "                    self.dLdV[i][j] = self.dLdO[j] * self.H[i]\n",
    "\n",
    "            # Calculate the gradient of the loss (L) wrt to K\n",
    "            self.dLdK = [ 0.0, 0.0, 0.0 ]\n",
    "            for i in range(len(self.dLdH)):\n",
    "                self.dLdK[i] = Activations.sigmoid(self.dLdH[i]) * Activations.sigmoid(1 - self.dLdH[i])\n",
    "\n",
    "            # Calculate the gradient of the loss (L) wrt to the bias B\n",
    "            self.dLdB = self.dLdK\n",
    "\n",
    "            # Calculate the gradient of the loss (L) wrt to weights W\n",
    "            self.dLdW = [\n",
    "              [0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0]\n",
    "            ]\n",
    "\n",
    "            for i in range(len(self.dLdW)):\n",
    "                for j in range(len(self.dLdK)):\n",
    "                    self.dLdW[i][j] = self.dLdK[j] * self.X[i]\n",
    "        except:\n",
    "            print(sys.exc_info())\n",
    "            print(self.Y)\n",
    "            print(self.T)\n",
    "\n",
    "    def step(self):\n",
    "        # Update C based on the learning rate and dL / dC\n",
    "        for i in range(len(self.C)):\n",
    "            self.C[i] = self.C[i] - self.learning_rate * self.dLdC[i]\n",
    "\n",
    "        # Update V based on the learning rate and dL / dV\n",
    "        for i in range(len(self.V)):\n",
    "            for j in range(len(self.V[i])):\n",
    "                self.V[i][j] = self.V[i][j] - self.learning_rate * self.dLdV[i][j]\n",
    "\n",
    "        # Update B based on the learning rate and dL / dB\n",
    "        for i in range(len(self.dLdB)):\n",
    "            self.B[i] = self.B[i] - self.learning_rate * self.dLdB[i]\n",
    "            \n",
    "        # Update W based on the learning rate and dL / dW\n",
    "        for i in range(len(self.W)):\n",
    "            for j in range(len(self.W[i])):\n",
    "                self.W[i][j] = self.W[i][j] - self.learning_rate * self.dLdW[i][j]\n",
    "\n",
    "    def train(self, data: tuple, epochs: int):\n",
    "        inputs = data[0]\n",
    "        targets = data[1]\n",
    "        \n",
    "        for num_epoch in range(epochs):\n",
    "            for i in range(len(inputs)):\n",
    "                self.set_input(inputs[i])\n",
    "\n",
    "                one_hot_encoded_target = [ 0.0, 0.0 ]\n",
    "                one_hot_encoded_target[targets[i]] = 1.0\n",
    "\n",
    "                self.set_target(one_hot_encoded_target)\n",
    "\n",
    "                self.forwards()\n",
    "\n",
    "                # calculate gradients\n",
    "                self.calculate_gradients()\n",
    "\n",
    "                # take a step on the gradients\n",
    "                self.step()\n",
    "                # return self.W, self.B, self.V, self.C\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-45cdf6a02ed8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# calculating the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-76cc8cc8ed66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, epochs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_encoded_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforwards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-76cc8cc8ed66>\u001b[0m in \u001b[0;36mforwards\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# Calculate Y = softmax(O)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mActivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# return the predicted value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-94c94445500c>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(current, values)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mdenominator\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "# epoch implies a full pass over the entire dataset\n",
    "epochs = 50\n",
    "learning_rate = 0.03\n",
    "\n",
    "data = load_synth()\n",
    "training_data = data[0]\n",
    "validation_data = data[1]\n",
    "\n",
    "network = Network()\n",
    "network.set_learning_rate(learning_rate)\n",
    "network.train(training_data, epochs)\n",
    "\n",
    "# calculating the loss\n",
    "print(training_data[0][0])\n",
    "print(network.predict(training_data[0][0]))\n",
    "print(training_data[1][0])\n",
    "\n",
    "print(training_data[0][1])\n",
    "print(training_data[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
